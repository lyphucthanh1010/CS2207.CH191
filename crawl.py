import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

base_url = "https://www.healthline.com/health/diabetes"
headers = {"User-Agent": "Mozilla/5.0 (X11; Linux x86_64)"}

# Gá»­i request Ä‘áº¿n trang chÃ­nh
response = requests.get(base_url, headers=headers)
soup = BeautifulSoup(response.text, "html.parser")

# Láº¥y danh sÃ¡ch link liÃªn quan Ä‘áº¿n 'diabetes'
links = []
for a in soup.find_all("a", href=True):
    href = a["href"]
    if "/health/" in href and "diabetes" in href and href.startswith("https"):
        links.append(href)
links = list(set(links))  # loáº¡i trÃ¹ng láº·p

print(f"ğŸ”— TÃ¬m tháº¥y {len(links)} bÃ i viáº¿t.")

# Crawl tá»«ng bÃ i
data = []
for link in links[:20]:  # crawl thá»­ 20 bÃ i
    try:
        time.sleep(1)
        res = requests.get(link, headers=headers, timeout=10)
        art = BeautifulSoup(res.text, "html.parser")
        title = art.find("h1").get_text(strip=True) if art.find("h1") else ""
        content = " ".join(p.get_text(strip=True) for p in art.find_all("p"))
        if len(content) > 200:
            data.append({"url": link, "title": title, "content": content})
            print(f"âœ… {title[:60]}...")
    except Exception as e:
        print(f"âš ï¸ Lá»—i {link}: {e}")

# LÆ°u thÃ nh dataset má»›i
df = pd.DataFrame(data)
df.to_csv("diabetes_text.csv", index=False)
print(f"ğŸ’¾ ÄÃ£ lÆ°u {len(df)} bÃ i viáº¿t táº¡i diabetes_text.csv")

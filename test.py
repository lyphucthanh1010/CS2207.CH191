"""
generate_synthetic_10k_fixed.py
PhiÃªn báº£n á»•n Ä‘á»‹nh khÃ´ng phá»¥ thuá»™c vÃ o dá»¯ liá»‡u NLTK.
- KhÃ´ng yÃªu cáº§u sdv hoáº·c nltk_data
- Tá»± Ä‘á»™ng fallback náº¿u thiáº¿u thÆ° viá»‡n
- Sinh 10,000 hÃ ng synthetic tá»« dá»¯ liá»‡u crawl vá» tiá»ƒu Ä‘Æ°á»ng
"""

import os
import random
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import NearestNeighbors

# ======================
# 1ï¸âƒ£ Cáº¥u hÃ¬nh ban Ä‘áº§u
# ======================
DATA_DIR = Path("data")
INPUT_TEXT = DATA_DIR / "diabetes_text.csv"
INPUT_FEATURES = DATA_DIR / "diabetes_text_features.csv"
OUT_NUMERIC = DATA_DIR / "diabetes_synthetic_numeric.csv"
OUT_TEXT = DATA_DIR / "diabetes_synthetic_text.csv"
OUT_FULL = DATA_DIR / "diabetes_synthetic_full.csv"

N_SYNTH = 10000
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

# ======================
# 2ï¸âƒ£ Äá»c dá»¯ liá»‡u gá»‘c
# ======================
if not INPUT_TEXT.exists() or not INPUT_FEATURES.exists():
    raise FileNotFoundError("âŒ Thiáº¿u file input: hÃ£y Ä‘áº£m báº£o cÃ³ diabetes_text.csv vÃ  diabetes_text_features.csv trong thÆ° má»¥c data/")

df_feats = pd.read_csv(INPUT_FEATURES)
df_text = pd.read_csv(INPUT_TEXT)

if "title" in df_feats.columns and "title" in df_text.columns:
    df = pd.merge(df_feats, df_text[["title", "content"]], on="title", how="left")
else:
    df = df_feats.copy()
    df["content"] = df_text["content"] if "content" in df_text.columns else ""

exclude = {"title", "content"}
numeric_cols = [c for c in df.columns if c not in exclude]
X_real = df[numeric_cols].fillna(0)

print(f"âœ… Äá»c {len(df)} hÃ ng gá»‘c, {len(numeric_cols)} cá»™t numeric.")

# ======================
# 3ï¸âƒ£ Sinh dá»¯ liá»‡u numeric 10,000 hÃ ng
# ======================
print("ðŸ§® Äang sinh dá»¯ liá»‡u numeric (bootstrap + jitter)...")
rows = []
for i in range(N_SYNTH):
    idx = np.random.randint(0, len(X_real))
    row = X_real.iloc[idx].astype(float).values.copy()
    noise = np.random.normal(0, 0.01, len(row))
    row = row + noise * (np.abs(row) + 1e-6)
    rows.append(row)
synth = pd.DataFrame(rows, columns=X_real.columns)
synth.to_csv(OUT_NUMERIC, index=False)
print(f"ðŸ’¾ LÆ°u: {OUT_NUMERIC} ({len(synth)} hÃ ng)")

# ======================
# 4ï¸âƒ£ Táº¡o ná»™i dung vÄƒn báº£n synthetic
# ======================
print("ðŸ§  Äang táº¡o ná»™i dung vÄƒn báº£n synthetic...")

contents = df["content"].fillna("").astype(str).tolist()
titles = df["title"].fillna("").astype(str).tolist()
n_orig = len(contents)

# fallback tokenizer khÃ´ng cáº§n nltk
def sent_tokenize(text):
    return [s.strip() for s in text.split(".") if len(s.strip()) > 0]

def synonym_replace(text):
    words = text.split()
    for i in range(len(words)):
        if random.random() < 0.05 and len(words[i]) > 4:
            words[i] = words[i][::-1]  # Ä‘áº£o chá»¯ lÃ m "nhiá»…u" nháº¹
    return " ".join(words)

def sentence_shuffle(text):
    sents = sent_tokenize(text)
    if len(sents) > 1 and random.random() < 0.4:
        random.shuffle(sents)
    return ". ".join(sents) + "."

# khá»Ÿi táº¡o TF-IDF Ä‘á»ƒ chá»n bÃ i gá»‘c gáº§n nháº¥t
vectorizer = TfidfVectorizer(max_features=300, stop_words="english")
tfidf = vectorizer.fit_transform(df["content"].fillna("").astype(str))
nn = NearestNeighbors(n_neighbors=1).fit(tfidf)

# táº¡o ná»™i dung synthetic
synth_text = []
for i in range(N_SYNTH):
    base_idx = np.random.randint(0, n_orig)
    base_title = titles[base_idx]
    base_content = contents[base_idx]

    new_text = sentence_shuffle(base_content)
    if random.random() < 0.6:
        new_text = synonym_replace(new_text)
    if random.random() < 0.05:
        other_idx = np.random.randint(0, n_orig)
        new_text += "\n\n" + contents[other_idx][:150]

    synth_text.append({
        "title": base_title + " [synthetic]" if random.random() < 0.6 else base_title,
        "content": new_text
    })

df_text_syn = pd.DataFrame(synth_text)
df_text_syn.to_csv(OUT_TEXT, index=False)
print(f"ðŸ’¾ LÆ°u: {OUT_TEXT} ({len(df_text_syn)} hÃ ng)")

# ======================
# 5ï¸âƒ£ Gá»™p full dataset
# ======================
df_full = pd.concat([synth.reset_index(drop=True), df_text_syn.reset_index(drop=True)], axis=1)
df_full.to_csv(OUT_FULL, index=False)
print(f"âœ… ÄÃ£ táº¡o full dataset: {OUT_FULL}")
print("ðŸŽ‰ HoÃ n táº¥t! Tá»•ng cá»™ng 10.000 hÃ ng synthetic.")
